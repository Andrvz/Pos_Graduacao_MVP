{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrvz/Pos_Graduacao_MVP/blob/main/MVP_2_Olist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Checklist Sugerido\n",
        "\n",
        "# 2.1 - Entender e descrever claramente o problema que está sendo resolvido.\n",
        "# Ok    # todo: 1. Qual é a descrição do problema?\n",
        "# Ok    # todo: 2. Você tem premissas ou hipóteses sobre o problema? Quais?\n",
        "# ok    # todo: 3. Que restrições ou condições foram impostas para selecionar os dados?\n",
        "# ok    # todo: 4. Descreva o seu dataset (atributos, imagens, anotações, etc).\n",
        "\n",
        "\n",
        "# 2.2 - Realizar operações de preparação dos dados.\n",
        "# Ok    # todo: 1. Preparação dos dados: dataset em raw e tratado\n",
        "# !!!   # todo: 2. Separação dos dados: em treino, teste e validação\n",
        "# !!!   # todo: 3. Avaliar aplicação de validação cruzada\n",
        "# Ok    # todo: 4. Refinar a quantidade de atributos disponíveis com feature selection\n",
        "\n",
        "\n",
        "# 2.3 - construir modelos para resolver o problema em questão\n",
        "# todo: 1. Ajuste inicial de hiperparâmetros\n",
        "# todo: 2. Análise de underfitting\n",
        "# todo: 3. Otimização de hiperparâmetros\n",
        "# todo: 4. Avaliar aplicação de outros métodos\n",
        "# todo: 5. Pode usar ensembles?\n",
        "\n",
        "\n",
        "# 2.4 - Analisar o desempenho dos modelos gerados em dados não vistos (com a base de teste)\n",
        "# todo: 1. Seleção de métricas de avaliação\n",
        "# todo: 2. Testar modelo com novas bases\n",
        "# todo: 3. Análise de resultados\n",
        "# todo: 4. Análise de overfitting\n",
        "# todo: 5. Conclusão"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Yw7YSCOnNey5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MVP - Análise de Dados de Vendas por E-comerce da Olist\n",
        "#### Desenvolvido por André Vaz em Julho de 2023\n",
        "\\\\\n",
        "\n",
        "\n",
        "**Disciplina:** Machine Learning & Analytics \\\\\n",
        "**Pós-Graduação:** Ciência de Dados e Analytics \\\\\n",
        "**Universidade:** PUC-Rio\n"
      ],
      "metadata": {
        "id": "PL4ngaabnrsn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "________________________________________________________________________________\n",
        "## Apresentação do Projeto"
      ],
      "metadata": {
        "id": "v1d86uFJyU61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A Olist é uma empresa brasileira de e-commerce que oferece uma plataforma de vendas para lojas de todos os portes. O dataset divulgado de forma pública pela mesma contém um conjunto de tabelas que reunem informações de seus clientes, vendas, pedidos, produtos e vendedores. Abordando o período entre os anos de 2016 à 2018, o dataset da Olist apresenta mais de 100.000 pedidos realizados por cerca de 32.000 clientes em vendas ditribuídas pelas 5 regiões do Brasil.\n",
        "\n",
        "- O projeto abordará o processo de extração, tratamento e carregamento do dataset. Assim, permitirá o desenvolvimento de estudos que utilizem técnicas de Machine Learning, como a análise de séries temporais e avaliação do perfil de clientes a fim de identificar padrões de compras e comportamentos dos consumidores, assim como fornecer insights para melhorias em marketing e atendimento ao cliente. Devido à limitação de memória para arquivos inseridos no repositório github, este projeto não abordará o dataset de geolocalização de vendas da Olist."
      ],
      "metadata": {
        "id": "Sm0TAe8to91Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Dataset **Customers**: dados sobre clientes.\n",
        "> **customer_id**: identificação do cliente por pedido. \\\n",
        "> **customer_unique_id**: identificação única do cliente. \\\n",
        "> **customer_zip_code_prefix**: código postal do cliente. \\\n",
        "> **customer_city**: cidade onde o cliente reside. \\\n",
        "> **customer_state**: estado onde o cliente reside.\n",
        "\n",
        "*   Dataset **Orders**: dados sobre vendas.\n",
        "> **order_id**: identificação única do pedido. \\\n",
        "> **customer_id**: identificação única do cliente que fez o pedido. \\\n",
        "> **order_status**: status do pedido. \\\n",
        "> **order_purchase_timestamp**: data e hora da compra. \\\n",
        "> **order_approved_at**: data e hora em que o pagamento foi aprovado. \\\n",
        "> **order_delivered_carrier_date**: data em que o pedido foi entregue à transportadora. \\\n",
        "> **order_delivered_customer_date**: data em que o pedido foi entregue ao cliente. \\\n",
        "> **order_estimated_delivery_date**: data estimada de entrega.\n",
        "\n",
        "*   Dataset **Order Items**: dados sobre pedidos por vendas.\n",
        "> **order_id**: identificação única do pedido. \\\n",
        "> **order_item_id**: identificação única do item do pedido. \\\n",
        "> **product_id**: identificação única do produto. \\\n",
        "> **seller_id**: identificação única do vendedor. \\\n",
        "> **shipping_limit_date**: data limite para o envio do produto. \\\n",
        "> **price**: preço unitário do produto. \\\n",
        "> **freight_value**: valor do frete.\n",
        "\n",
        "*   Dataset **Order Payments**: dados sobre pagamentos por vendas.\n",
        "> **order_id**: identificação única do pedido. \\\n",
        "> **payment_sequential**: número de sequência do pagamento em um determinado pedido. \\\n",
        "> **payment_type**: tipo de pagamento utilizado. \\\n",
        "> **payment_installments**: número de parcelas em que o pagamento foi dividido. \\\n",
        "> **payment_value**: valor total pago.\n",
        "\n",
        "*   Dataset **Order Reviews**: dados sobre avaliações por vendas.\n",
        "> **review_id**: identificação única da avaliação. \\\n",
        "> **order_id**: identificação única do pedido. \\\n",
        "> **review_score**: pontuação da avaliação (0 a 5). \\\n",
        "> **review_comment_title**: título presente na mensagem de avaliação.\\\n",
        "> **review_comment_message**: mensagem de avaliação.\\\n",
        "> **review_creation_date**: data de criação da avaliação. \\\n",
        "> **review_answer_timestamp**: data em que a resposta à avaliação foi enviada.\n",
        "\n",
        "*   Dataset **Products**: dados sobre produtos.\n",
        "> **product_id**: identificação única do produto. \\\n",
        "> **product_category_name**: categoria do produto. \\\n",
        "> **product_name_lenght**: comprimento em caracteres do nome do produto. \\\n",
        "> **product_description_lenght**: comprimento da descrição do produto. \\\n",
        "> **product_photos_qty**: quantidade de fotos do produto. \\\n",
        "> **product_weight_g**: peso do produto em gramas. \\\n",
        "> **product_length_cm**: comprimento do produto em centímetros. \\\n",
        "> **product_height_cm**: altura do produto em centímetros. \\\n",
        "> **product_width_cm**: largura do produto em centímetros.\n",
        "\n",
        "*   Dataset **Sellers**: dados sobre vendedores.\n",
        "> **seller_id**: identificação única do vendedor. \\\n",
        "> **seller_zip_code_prefix**: código postal do vendedor. \\\n",
        "> **seller_city**: cidade onde o vendedor está localizado. \\\n",
        "> **seller_state**: estado onde o vendedor está localizado."
      ],
      "metadata": {
        "id": "v5KnJT8IpUFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "________________________________________________________________________________\n",
        "\n",
        "## Códigos Python"
      ],
      "metadata": {
        "id": "2QZTf87vylEd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title 1.1 - Instalações Iniciais\n",
        "\n",
        "!pip install pandas numpy                                                     # Instalando bibliotecas básicas para cicência de dados\n",
        "!pip install holoviews seaborn bokeh matplotlib                               # Instalando bibliotecas de visualização\n",
        "!pip install ipywidgets                                                       # Instalando biblioteca para uso de widgets no notebook\n",
        "!jupyter nbextension enable --py widgetsnbextension                           # Instalando extensão para uso de widgets no notebook\n",
        "!pip install summarytools                                                     # Instalando biblioteca para widget de resumo de dados"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-07-22T14:21:04.331585Z",
          "end_time": "2023-07-22T14:21:04.337695Z"
        },
        "id": "4SgzdV6auXFC",
        "cellView": "form"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "source": [
        "#@title 1.2 - Importações\n",
        "\n",
        "# Importações gerais\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "from datetime import date, datetime, timedelta\n",
        "from summarytools import dfSummary\n",
        "from openpyxl import Workbook\n",
        "\n",
        "# Importações de bibliotecas de visualização\n",
        "import holoviews as hv\n",
        "from holoviews import opts\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from bokeh.models.tools import HoverTool\n",
        "from ipywidgets import widgets\n",
        "from IPython.display import display\n",
        "import missingno as ms # para tratamento de missings\n",
        "\n",
        "# Ignorar avisos\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "lGJSl_-vuXFD",
        "ExecuteTime": {
          "start_time": "2023-07-23T03:49:23.603073Z",
          "end_time": "2023-07-23T03:49:23.617135Z"
        },
        "cellView": "form"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.3 - Métodos utilizados para ETL\n",
        "class ETL:\n",
        "    def __init__(self, tipo):\n",
        "        self.tipo = tipo\n",
        "\n",
        "        self.dataset_raw = {}\n",
        "        self.resumo_tabelas_raw = None\n",
        "        self.resumo_colunas_raw = None\n",
        "\n",
        "        self.dataset = {}\n",
        "        self.resumo_tabelas = None\n",
        "        self.resumo_colunas = None\n",
        "\n",
        "        self.pipeline()\n",
        "\n",
        "    def carregar_dados(self):\n",
        "\n",
        "        self.dataset_raw = {'customer': pd.read_csv(\n",
        "            'https://raw.githubusercontent.com/Andrvz/mvp_sprint_1/main/Dataset/olist_customers_dataset.csv'),\n",
        "            'orders': pd.read_csv(\n",
        "                'https://raw.githubusercontent.com/Andrvz/mvp_sprint_1/main/Dataset/olist_orders_dataset.csv'),\n",
        "            'order_items': pd.read_csv(\n",
        "                'https://raw.githubusercontent.com/Andrvz/mvp_sprint_1/main/Dataset/olist_order_items_dataset.csv'),\n",
        "            'order_payments': pd.read_csv(\n",
        "                'https://raw.githubusercontent.com/Andrvz/mvp_sprint_1/main/Dataset/olist_order_payments_dataset.csv'),\n",
        "            'order_reviews': pd.read_csv(\n",
        "                'https://raw.githubusercontent.com/Andrvz/mvp_sprint_1/main/Dataset/olist_order_reviews_dataset.csv'),\n",
        "            'products': pd.read_csv(\n",
        "                'https://raw.githubusercontent.com/Andrvz/mvp_sprint_1/main/Dataset/olist_products_dataset.csv'),\n",
        "            'sellers': pd.read_csv(\n",
        "                'https://raw.githubusercontent.com/Andrvz/mvp_sprint_1/main/Dataset/olist_sellers_dataset.csv')}\n",
        "\n",
        "    def tratar_dados(self):\n",
        "        self.dataset = {}\n",
        "\n",
        "        for data in self.dataset_raw.keys():\n",
        "            if data == 'customer':\n",
        "                df = self.dataset_raw[f'{data}']\n",
        "                df = df.astype({'customer_id': 'object',\n",
        "                                'customer_unique_id': 'object',\n",
        "                                'customer_zip_code_prefix': 'int',\n",
        "                                'customer_city': 'category',\n",
        "                                'customer_state': 'category'})\n",
        "                df.dropna(inplace=True)\n",
        "                df.drop(columns=[], inplace=True)\n",
        "                self.dataset[f'{data}'] = df\n",
        "                del df\n",
        "            if data == 'order_items':\n",
        "                df = self.dataset_raw[f'{data}']\n",
        "                df = df.astype({'order_id': 'object',\n",
        "                                'order_item_id': 'int',\n",
        "                                'product_id': 'object',\n",
        "                                'seller_id': 'object',\n",
        "                                'shipping_limit_date': 'datetime64[ns]',\n",
        "                                'price': 'float',\n",
        "                                'freight_value': 'float'})\n",
        "\n",
        "                df.dropna(inplace=True)\n",
        "                df.drop(columns=[], inplace=True)\n",
        "                self.dataset[f'{data}'] = df\n",
        "                del df\n",
        "            if data == 'orders':\n",
        "                df = self.dataset_raw[f'{data}']\n",
        "\n",
        "                df = df.assign(\n",
        "                    purchase_date=pd.to_datetime(df['order_purchase_timestamp']).dt.date,\n",
        "                    purchase_year=pd.to_datetime(df['order_purchase_timestamp']).dt.year,\n",
        "                    purchase_month=pd.to_datetime(df['order_purchase_timestamp']).dt.month,\n",
        "                    purchase_day=pd.to_datetime(df['order_purchase_timestamp']).dt.day_name(),\n",
        "                    purchase_hour=pd.to_datetime(df['order_purchase_timestamp']).dt.hour)\n",
        "\n",
        "                def time_period(x):\n",
        "                    if x >= 5 and x < 12:\n",
        "                        return \"Morning\"\n",
        "                    elif x >= 12 and x < 17:\n",
        "                        return \"Afternoon\"\n",
        "                    elif x >= 17 and x < 21:\n",
        "                        return \"Evening\"\n",
        "                    else:\n",
        "                        return \"Night\"\n",
        "\n",
        "                df[\"purchase_time\"] = df[\"purchase_hour\"].apply(time_period)\n",
        "\n",
        "                df = df.astype({'order_id': 'object',\n",
        "                                'customer_id': 'object',\n",
        "                                'order_status': 'category',\n",
        "                                'order_purchase_timestamp': 'datetime64[ns]',\n",
        "                                'order_approved_at': 'datetime64[ns]',\n",
        "                                'order_delivered_carrier_date': 'datetime64[ns]',\n",
        "                                'order_delivered_customer_date': 'datetime64[ns]',\n",
        "                                'order_estimated_delivery_date': 'datetime64[ns]'})\n",
        "\n",
        "                df.dropna(inplace=True)\n",
        "                df.drop(columns=[], inplace=True)\n",
        "                self.dataset[f'{data}'] = df\n",
        "                del df\n",
        "            if data == 'order_payments':\n",
        "                df = self.dataset_raw[f'{data}']\n",
        "                df = df.astype({'order_id': 'object',\n",
        "                                'payment_sequential': 'int',\n",
        "                                'payment_type': 'category',\n",
        "                                'payment_installments': 'int',\n",
        "                                'payment_value': 'float'})\n",
        "\n",
        "                df.dropna(inplace=True)\n",
        "                df.drop(columns=[], inplace=True)\n",
        "                self.dataset[f'{data}'] = df\n",
        "                del df\n",
        "            if data == 'order_reviews':\n",
        "                df = self.dataset_raw[f'{data}']\n",
        "                df = df.astype({'review_id':'object',\n",
        "                                'order_id':'object',\n",
        "                                 'review_score':'int',\n",
        "                                'review_creation_date':'datetime64[ns]',\n",
        "                                'review_answer_timestamp':'datetime64[ns]'})\n",
        "\n",
        "                df.dropna(inplace=True)\n",
        "                df.drop(columns=[], inplace=True)\n",
        "                self.dataset[f'{data}'] = df\n",
        "                del df\n",
        "            if data == 'products':\n",
        "                df = self.dataset_raw[f'{data}']\n",
        "                df['product_width_cm'] = df['product_width_cm'].fillna(0)\n",
        "                df['product_height_cm'] = df['product_height_cm'].fillna(0)\n",
        "                df['product_length_cm'] = df['product_length_cm'].fillna(0)\n",
        "                df['product_weight_g'] = df['product_weight_g'].fillna(0)\n",
        "                df['product_name_lenght'] = df['product_name_lenght'].fillna(0)\n",
        "                df['product_description_lenght'] = df['product_description_lenght'].fillna(0)\n",
        "                df['product_photos_qty'] = df['product_photos_qty'].fillna(0)\n",
        "                df['product_category_name'] = df['product_category_name'].fillna('Sem Categoria')\n",
        "\n",
        "                df = df.astype({'product_id': 'object',\n",
        "                                'product_category_name': 'category',\n",
        "                                'product_name_lenght': 'float',\n",
        "                                'product_description_lenght': 'float',\n",
        "                                'product_photos_qty': 'float',\n",
        "                                'product_weight_g': 'float',\n",
        "                                'product_length_cm': 'float',\n",
        "                                'product_height_cm': 'float',\n",
        "                                'product_width_cm': 'float'})\n",
        "\n",
        "                df.dropna(inplace=True)\n",
        "                df.drop(columns=[], inplace=True)\n",
        "                self.dataset[f'{data}'] = df\n",
        "                del df\n",
        "            if data == 'sellers':\n",
        "                df = self.dataset_raw[f'{data}']\n",
        "                df = df.astype({'seller_id': 'object',\n",
        "                                'seller_zip_code_prefix': 'int',\n",
        "                                'seller_city': 'category',\n",
        "                                'seller_state': 'category'})\n",
        "\n",
        "                df.dropna(inplace=True)\n",
        "                df.drop(columns=[], inplace=True)\n",
        "                self.dataset[f'{data}'] = df\n",
        "                del df\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def tabela_consolidada(dict_dataset):\n",
        "        df = dict_dataset['order_items']\n",
        "        df = pd.merge(df, dict_dataset['products'],on='product_id')\n",
        "        df = pd.merge(df, dict_dataset['sellers'],on='seller_id')\n",
        "        df = pd.merge(df, dict_dataset['orders'],on='order_id')\n",
        "        df = pd.merge(df, dict_dataset['customer'],on='customer_id')\n",
        "        df = pd.merge(df, dict_dataset['order_reviews'],on='order_id')\n",
        "        df = pd.merge(df, dict_dataset['order_payments'],on='order_id')\n",
        "        return df\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def resumo_dados(dicionario_tabelas):\n",
        "        tabelas = []\n",
        "        titles = []\n",
        "        resumo_colunas = pd.DataFrame({},)\n",
        "        for nome_tabela in dicionario_tabelas:\n",
        "            tabelas.append(dicionario_tabelas[nome_tabela])\n",
        "            titles.append(nome_tabela)\n",
        "            df = HUB.resumo_dados(dicionario_tabelas[nome_tabela])[0]\n",
        "            df[\"Tabela\"] = df.apply(lambda row: nome_tabela, axis=1)\n",
        "            resumo_colunas = pd.concat([resumo_colunas, df])\n",
        "\n",
        "        resumo_tabelas = pd.DataFrame({},)\n",
        "        resumo_tabelas['datasets']= titles\n",
        "        resumo_tabelas['columns'] = [', '.join([col for col, null in data.isnull().sum().items() ]) for data in tabelas]\n",
        "        resumo_tabelas['Linhas']= [data.shape[0] for data in tabelas]\n",
        "        resumo_tabelas['Colunas']= [data.shape[1] for data in tabelas]\n",
        "        resumo_tabelas['Duplicadas']= [len(data[data.duplicated()]) for data in tabelas]\n",
        "        resumo_tabelas['Linhas Nulas']= [data.isnull().sum().sum() for data in tabelas]\n",
        "        resumo_tabelas['Colunas Nulas'] = [', '.join([col for col, null in data.isnull().sum().items() if null > 0]) for data in tabelas]\n",
        "        resumo_tabelas.style.background_gradient(cmap='YlGnBu')\n",
        "        return resumo_tabelas, resumo_colunas\n",
        "\n",
        "    def pipeline(self):\n",
        "        path = None\n",
        "        if self.tipo == \"Colab\":\n",
        "            print(\"Ambiente Python: Google Colab\")\n",
        "            path = \"/data/\"\n",
        "\n",
        "        if self.tipo == \"DataSpell\":\n",
        "            print(\"Ambiente Python: Dataspell\")\n",
        "            path = \"data/\"\n",
        "\n",
        "        self.carregar_dados()\n",
        "        self.tratar_dados()\n",
        "        self.dataset_raw['consolidado'] = ETL.tabela_consolidada(self.dataset_raw)\n",
        "        self.resumo_tabelas_raw, self.resumo_colunas_raw = ETL.resumo_dados(self.dataset_raw)\n",
        "        self.dataset['consolidado'] = ETL.tabela_consolidada(self.dataset)\n",
        "        self.resumo_tabelas, self.resumo_colunas = ETL.resumo_dados(self.dataset)\n",
        "\n",
        "        if path != None:\n",
        "            ETL.salvar_dados_csv(self.dataset_raw, path, tipo=\"raw\")\n",
        "            ETL.salvar_dados_csv(self.dataset, path, tipo=\"processed\")\n",
        "\n",
        "    @staticmethod\n",
        "    def salvar_dados_csv(data, path, tipo=None):\n",
        "        # Create a directory to store the CSV files\n",
        "        print(\"Datasets salvos em \" + path + tipo)\n",
        "        if not os.path.exists(path + tipo):\n",
        "            os.makedirs(path + tipo)\n",
        "\n",
        "        for nome_aba, df in data.items():\n",
        "            # Convert the DataFrame to a CSV file\n",
        "            filename = f'{path + tipo}/{nome_aba}.csv'\n",
        "            df.to_csv(filename, index=False)\n"
      ],
      "metadata": {
        "id": "T9FJNok8utY4",
        "ExecuteTime": {
          "start_time": "2023-07-23T04:40:14.641664Z",
          "end_time": "2023-07-23T04:40:14.660912Z"
        },
        "cellView": "form"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.4 - Métodos utilizados para Snippets\n",
        "class HUB:\n",
        "\n",
        "  @staticmethod\n",
        "  def hist(dataset):\n",
        "    dataset_dict = {}\n",
        "    for n in dataset.keys():\n",
        "        dataset_dict[n] = dataset[n].columns\n",
        "\n",
        "    def dados_graficos(df, col, datatype):\n",
        "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 3))\n",
        "\n",
        "        if datatype == 'numeric':\n",
        "            sns.boxplot(x=col, data=df, ax=ax1, orient='v', color='darkslategrey')\n",
        "            sns.distplot(df[col], ax=ax2, color='teal').set_title(f'Histograma')\n",
        "            ax1.set_title('Boxplot', fontsize=14, color='black')\n",
        "            ax1.set_xlabel('Box', fontsize=14, color='black')\n",
        "            ax1.set_ylabel(col + ' Values', fontsize=14, color='black')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def update_dropdown(*args):\n",
        "        selected_key = dropdown_key.value\n",
        "        selected_type = dropdown_type.value\n",
        "\n",
        "        # Map the data type strings to corresponding data types\n",
        "        if selected_type == 'numeric':\n",
        "            dropdown_value.options = dataset[selected_key].select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "\n",
        "    def select(change):\n",
        "        output.clear_output()\n",
        "        if change.new:\n",
        "            selected_type = dropdown_type.value\n",
        "            with output:\n",
        "               dados_graficos(dataset[dropdown_key.value], col=change.new, datatype=selected_type)\n",
        "\n",
        "    output = widgets.Output()\n",
        "\n",
        "    dropdown_key = widgets.Dropdown(options=list(dataset.keys()), description='Tabela:')\n",
        "    dropdown_type = widgets.Dropdown(options=['numeric'], description='Tipo:')\n",
        "    dropdown_value = widgets.Dropdown(description='Coluna:')\n",
        "\n",
        "    dropdown_key.observe(update_dropdown, 'value')\n",
        "    dropdown_value.observe(select, names=\"value\")\n",
        "    dropdown_type.observe(update_dropdown, 'value')\n",
        "\n",
        "    display(dropdown_key, dropdown_type, dropdown_value)\n",
        "    display(output)\n",
        "\n",
        "  @staticmethod\n",
        "  def relatorio_tabelas(df, title):\n",
        "    null_counts = df.isnull().sum()\n",
        "    null_counts = null_counts[null_counts > 0]\n",
        "    null_counts = null_counts.sort_values(ascending=False)\n",
        "\n",
        "    print(f\"\"\"\n",
        "Tabela: {title}\n",
        "Linhas: {df.shape[0]}\n",
        "Colunas: {df.shape[1]}\n",
        "Total de nulos:  {df.isnull().sum().sum()}\n",
        "----------------------------------------------\n",
        "Colunas nulas:\n",
        "{null_counts}\n",
        "  \"\"\")\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def resumo_dados(data):\n",
        "      \"\"\"\n",
        "      :param data: pandas dataframe.\n",
        "      :return: pandas dataframe com tipo, quantidade, valores distintos, valores nulos (total e porcentagem).\n",
        "      \"\"\"\n",
        "      if isinstance(data, pd.DataFrame):\n",
        "          data_info = pd.DataFrame({})\n",
        "          data_info['Nulos'] = round(data.isna().sum(), 2)\n",
        "          data_info['%_Nulos'] = round((data.isna().sum()) / (data.shape[0]) * 100, 1).astype(str) + '%'\n",
        "          data_info['Tipo'] = data.dtypes.tolist()\n",
        "          data_info['Valores_Dist'] = data.nunique().tolist()\n",
        "          data_info['Itens'] = data.count()\n",
        "\n",
        "          data_info = data_info.reindex(columns=['Tipo', 'Itens', 'Valores_Dist', 'Nulos', '%_Nulos'])\n",
        "          data_info = data_info.sort_values(by='%_Nulos', ascending=False)\n",
        "          return data_info, data_info.reset_index()\n",
        "      if isinstance(data, dict):\n",
        "          data_info_dict = pd.DataFrame({})\n",
        "          for key in data.keys():\n",
        "              data_info = pd.DataFrame({})\n",
        "              data_info['Nulos'] = round(key.isna().sum(), 2)\n",
        "              data_info['%_Nulos'] = round((key.isna().sum()) / (key.shape[0]) * 100, 1).astype(str) + '%'\n",
        "              data_info['Tipo'] = key.dtypes.tolist()\n",
        "              data_info['Valores_Dist'] = key.nunique().tolist()\n",
        "              data_info['Itens'] = key.count()\n",
        "              data_info['Duplicados'] = data[key].duplicated().sum()\n",
        "\n",
        "              data_info = data_info.reindex(columns=['Tipo', 'Itens', 'Valores_Dist', 'Nulos', '%_Nulos','Duplicados'])\n",
        "              data_info_dict = pd.concat([data_info_dict, data_info])\n",
        "              return data_info_dict, data_info_dict.reset_index()\n",
        "      if not isinstance(data, (pd.DataFrame, dict)):\n",
        "          print(\"Insira um argumento do tipo 'pd.DataFrame' ou 'dict'\")\n",
        "\n",
        "  @staticmethod\n",
        "  def resumo_dados_categoricos(data):\n",
        "      \"\"\"\n",
        "      :param data: pd.DataFrame ou pd.Series\n",
        "      :return: pandas dataframe com frequência e proporção de valores distintos.\n",
        "      \"\"\"\n",
        "      if isinstance(data, pd.DataFrame):\n",
        "          columns = data.select_dtypes(include=['category']).columns.tolist()\n",
        "          data_info = pd.DataFrame()\n",
        "          data_info['Frequência'] = data[columns].apply(lambda x: x.value_counts()).T.stack()\n",
        "          data_info['Proporção'] = data[columns].apply(\n",
        "              lambda x: x.value_counts(normalize=True).mul(100).round(1).astype(str) + '%').T.stack()\n",
        "          return data_info, data_info.reset_index()\n",
        "      if isinstance(data, pd.Series):\n",
        "          data_info = pd.DataFrame()\n",
        "          data_info['Frequência'] = data.value_counts().T\n",
        "          data_info['Proporção'] = (data.value_counts(normalize=True).mul(100).round(1).astype(str) + '%').T\n",
        "          return data_info, data_info.reset_index()\n",
        "      if not isinstance(data, (pd.DataFrame, pd.Series)):\n",
        "          print(\"Insira um argumento do tipo 'pd.DataFrame' ou 'pd.Series'\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gS7tu1KFA2uB",
        "ExecuteTime": {
          "start_time": "2023-07-23T03:47:45.827919Z",
          "end_time": "2023-07-23T03:47:45.868455Z"
        },
        "cellView": "form"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.5 - Métodos utilizados para Widgets\n",
        "class VIZ:\n",
        "    @staticmethod\n",
        "    def widget_from_dict(dict_df):\n",
        "        from ipywidgets import widgets\n",
        "        from IPython.display import display\n",
        "\n",
        "        dropdown = widgets.ToggleButtons(\n",
        "            options=list(dict_df.keys()),\n",
        "            description='Dataset:',\n",
        "            disabled=False,\n",
        "            button_style='success')#,\n",
        "            # tooltips=['Dataset de clientes da Olist.', 'Dataset de geolozalização dos clientes da Olist.',\n",
        "                      # 'Dataset das vendas feitas pela Olist.', 'Dataset dos itens vendidos por venda feita pela Olist.',\n",
        "                      # 'Dataset dos métodos de pagamento das vendas feitas pela Olist.','Dataset de avaliações das vendas feitas pela Olist.',\n",
        "                      # 'Dataset de produtos vendidos pela Olist.', 'Dataset de vendedores parceiros da Olist.'])\n",
        "\n",
        "        output_1 = widgets.Output()\n",
        "        output_2 = widgets.Output()\n",
        "        output_3 = widgets.Output()\n",
        "        output_4 = widgets.Output()\n",
        "        output_5 = widgets.Output()\n",
        "\n",
        "        tab = widgets.Tab([output_1, output_2,output_3, output_4, output_5])\n",
        "        tab.set_title(0, 'Colunas')\n",
        "        tab.set_title(1, 'Amostra de 10 linhas')\n",
        "        tab.set_title(2, 'Describe')\n",
        "        tab.set_title(3, 'Resumo da Tabela')\n",
        "        tab.set_title(4, 'Colunas Categóricas')\n",
        "\n",
        "        def select(change):\n",
        "            output_1.clear_output()\n",
        "            output_2.clear_output()\n",
        "            output_3.clear_output()\n",
        "            output_4.clear_output()\n",
        "            output_5.clear_output()\n",
        "\n",
        "            if change.new in list(dict_df.keys()):\n",
        "                with output_1:\n",
        "                    display(HUB.resumo_dados(dict_df[change.new])[0])\n",
        "                with output_2:\n",
        "                    display(dict_df[change.new].sample(10))\n",
        "                with output_3:\n",
        "                    display(dict_df[change.new].describe())\n",
        "                with output_4:\n",
        "                    display(HUB.relatorio_tabelas(dict_df[change.new],change.new)[0])\n",
        "                with output_5:\n",
        "                    display(HUB.resumo_dados_categoricos(dict_df[change.new])[0])\n",
        "\n",
        "        dropdown.observe(select, names=\"value\")\n",
        "        return display(dropdown, tab)\n",
        "\n",
        "    @staticmethod\n",
        "    def widget_from_df(wid_data):\n",
        "\n",
        "        output_1 = widgets.Output()\n",
        "        output_2 = widgets.Output()\n",
        "        output_3 = widgets.Output()\n",
        "        output_4 = widgets.Output()\n",
        "\n",
        "        tab = widgets.Tab([output_1, output_2, output_3, output_4])\n",
        "        tab.set_title(0, 'Informações')\n",
        "        tab.set_title(1, 'Primeiras 5 linhas')\n",
        "        tab.set_title(2, 'Últimas 5 linhas')\n",
        "        tab.set_title(3, 'Coluna Categóricas')\n",
        "\n",
        "        with output_1:\n",
        "            display(HUB.resumo_dados(wid_data)[0])\n",
        "        with output_2:\n",
        "            display(wid_data.head(5))\n",
        "        with output_3:\n",
        "            display(wid_data.tail(5))\n",
        "        with output_4:\n",
        "            display(HUB.resumo_dados_categoricos(wid_data))\n",
        "\n",
        "        return display(tab)\n",
        "\n",
        "    @staticmethod\n",
        "    def widget_tabela(dataset_dict):\n",
        "      def execute_function(change):\n",
        "          output.clear_output()\n",
        "          if change.new:\n",
        "              selected_key = dropdown_key.value\n",
        "              with output:\n",
        "                  return print(dataset_dict[selected_key])\n",
        "\n",
        "\n",
        "      output = widgets.Output()\n",
        "\n",
        "      dropdown_key = widgets.Dropdown(options=list(dataset_dict.keys()), description='Tabela:')\n",
        "\n",
        "      dropdown_key.observe(execute_function, 'value')\n",
        "\n",
        "      display(dropdown_key)\n",
        "      display(output)\n",
        "\n",
        "    @staticmethod\n",
        "    def widget_visualizar_dataset(dict_df):\n",
        "      dropdown = widgets.ToggleButtons(\n",
        "      options=list(dict_df.keys()),\n",
        "      description='Dataset:',\n",
        "      disabled=False,\n",
        "      button_style='success')\n",
        "\n",
        "      output_1 = widgets.Output()\n",
        "      output_2 = widgets.Output()\n",
        "      output_3 = widgets.Output()\n",
        "      output_4 = widgets.Output()\n",
        "      output_5 = widgets.Output()\n",
        "\n",
        "      tab = widgets.Tab([output_1, output_2,output_3, output_4, output_5])\n",
        "      tab.set_title(0, 'Informações')\n",
        "      tab.set_title(1, 'Primeiras 5 linhas')\n",
        "      tab.set_title(2, 'Últimas 5 linhas')\n",
        "      tab.set_title(3, 'Colunas Numéricas')\n",
        "      tab.set_title(4, 'Colunas Categóricas')\n",
        "\n",
        "      def select(change):\n",
        "        output_1.clear_output()\n",
        "        output_2.clear_output()\n",
        "        output_3.clear_output()\n",
        "        output_4.clear_output()\n",
        "        output_5.clear_output()\n",
        "\n",
        "        if change.new in list(dict_df.keys()):\n",
        "          with output_1:\n",
        "            display(HUB.resumo_dados(dict_df[change.new])[0])\n",
        "          with output_2:\n",
        "            display(dict_df[change.new].head(5))\n",
        "          with output_3:\n",
        "            display(dict_df[change.new].tail(5))\n",
        "          with output_4:\n",
        "            display(dict_df[change.new].describe())\n",
        "          with output_5:\n",
        "            display(HUB.resumo_dados_categoricos(dict_df[change.new])[0])\n",
        "\n",
        "      dropdown.observe(select, names=\"value\")\n",
        "      return display(dropdown, tab)"
      ],
      "metadata": {
        "id": "pNp1cZpF_I3z",
        "ExecuteTime": {
          "start_time": "2023-07-23T03:47:46.527454Z",
          "end_time": "2023-07-23T03:47:46.548746Z"
        },
        "cellView": "form"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ORGANIZZAR - Resumo de tabelas do dataset\n",
        "\n",
        "def relatorio_tabelas(df, title):\n",
        "    null_counts = df.isnull().sum()\n",
        "    null_counts = null_counts[null_counts > 0]\n",
        "    null_counts = null_counts.sort_values(ascending=False)\n",
        "\n",
        "    print(f\"\"\"\n",
        "Tabela: {title}\n",
        "Linhas: {df.shape[0]}\n",
        "Colunas: {df.shape[1]}\n",
        "Total de nulos:  {df.isnull().sum().sum()}\n",
        "----------------------------------------------\n",
        "Colunas nulas:\n",
        "{null_counts}\n",
        "\"\"\")\n",
        "\n",
        "def relatorio_tabelas_consolidado(df, title):\n",
        "\n",
        "  print(f\"\"\"\n",
        "\n",
        "Tabela: {title}\n",
        "Linhas: {Olist_dataset['Consolidado'].shape[0]}\n",
        "Colunas: {Olist_dataset['Consolidado'].shape[1]}\n",
        "Total de nulos:  {Olist_dataset['Consolidado'].isnull().sum().sum()}\n",
        "Duplicados:\n",
        "\n",
        "----------------------------------------------\n",
        "Dia primeria venda: {Olist_dataset['Consolidado']['order_purchase_timestamp'].min()}\n",
        "Dia última venda: {Olist_dataset['Consolidado']['order_purchase_timestamp'].max()}\n",
        "\"\"\")\n",
        "\n",
        "def create_widgets_and_execute_function(dataset_dict):\n",
        "    def execute_function(change):\n",
        "        output.clear_output()\n",
        "        if change.new:\n",
        "            selected_key = dropdown_key.value\n",
        "            with output:\n",
        "                return print(relatorio_tabelas(df=dataset_dict[selected_key], title=selected_key))\n",
        "\n",
        "\n",
        "    output = widgets.Output()\n",
        "\n",
        "    dropdown_key = widgets.Dropdown(options=list(dataset_dict.keys()), description='Tabela:')\n",
        "\n",
        "    dropdown_key.observe(execute_function, 'value')\n",
        "\n",
        "    display(dropdown_key)\n",
        "    display(output)"
      ],
      "metadata": {
        "id": "SdYormaME4U0",
        "cellView": "form"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ORGANIZZAR - Widget para printar tabela em dicionario de dataframe\n",
        "# todo:\n",
        "def widget_tabela(dataset_dict):\n",
        "    def execute_function(change):\n",
        "        output.clear_output()\n",
        "        if change.new:\n",
        "            selected_key = dropdown_key.value\n",
        "            with output:\n",
        "                return print(dataset_dict[selected_key])\n",
        "\n",
        "\n",
        "    output = widgets.Output()\n",
        "\n",
        "    dropdown_key = widgets.Dropdown(options=list(dataset_dict.keys()), description='Tabela:')\n",
        "\n",
        "    dropdown_key.observe(execute_function, 'value')\n",
        "\n",
        "    display(dropdown_key)\n",
        "    display(output)"
      ],
      "metadata": {
        "id": "ZJsUUwFRSciQ",
        "cellView": "form"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.6 - Métodos utilizados para Machine learning\n"
      ],
      "metadata": {
        "id": "y2hb4Vs3jjax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "________________________________________________________________________________\n",
        "\n",
        "## Extração, Tratamento e Exploração do Dataset"
      ],
      "metadata": {
        "id": "t7fwDG21jslK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Inicialização da classe Olist\n",
        "\n",
        "# Inicializar o projeto\n",
        "Olist = ETL(tipo=\"Colab\")\n",
        "Olist.resumo_tabelas_raw\n",
        "# Tabelas salvas\n",
        "print(f\"\"\"\n",
        "- Confirmar se arquivos estão sendo salvos\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "K3G1luvz-Kp_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "________________________________________________________________________________\n",
        "\n",
        "### Dataset Raw"
      ],
      "metadata": {
        "id": "mumlyy-2j9bH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset Raw - Resumo de tabelas\n",
        "create_widgets_and_execute_function(Olist.dataset_raw)\n",
        "Olist.resumo_tabelas_raw"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mrx24F5GiQsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset Raw - Resumo de colunas\n",
        "Olist.resumo_colunas_raw.query('Tabela == \"orders\"')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YG7qy1QfdvEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset Raw - Consulta às tabelas\n",
        "VIZ.widget_visualizar_dataset({key: Olist.dataset_raw[key] for key in ['customer', 'orders', 'order_items', 'order_payments', 'order_reviews', 'products', 'sellers','consolidado']})"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lK0vfio_et08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "________________________________________________________________________________\n",
        "\n",
        "### Dataset Tratado"
      ],
      "metadata": {
        "id": "NjD6yxeVkArP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset Tratado - Resumo de tabelas\n",
        "create_widgets_and_execute_function(Olist.dataset)\n",
        "Olist.resumo_tabelas"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uKZikNRUiegs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset Tratado - Resumo de colunas\n",
        "Olist.resumo_colunas.query('Tabela == \"orders\"')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CVdXVkKAi0Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset Tratado - Consulta às tabelas\n",
        "VIZ.widget_visualizar_dataset({key: Olist.dataset[key] for key in ['customer', 'orders', 'order_items', 'order_payments', 'order_reviews', 'products', 'sellers','consolidado']})\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6hPw32WohWaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset Tratado - Explorando colunas numéricas das tabelas\n",
        "HUB.hist(Olist.dataset)"
      ],
      "metadata": {
        "id": "FXhJEjm6aTVF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "________________________________________________________________________________\n",
        "\n",
        "## Análise de RMF (Recência, Frequência e Valor Monetário)"
      ],
      "metadata": {
        "id": "L7DBAoykkI7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.1 - RMF Analysis\n",
        "# todo: https://www.kaggle.com/code/drindeng/detailed-rfm-analysis-and-k-means-clustering\n",
        "\n",
        "def RMF_Analysis(df):\n",
        "  # todo: recência\n",
        "  present_day = df['order_purchase_timestamp'].max() + timedelta(days=2)\n",
        "  recency_df= pd.DataFrame(df.groupby(by='customer_unique_id', as_index=False)['order_purchase_timestamp'].max())\n",
        "  recency_df['Recency']= recency_df['order_purchase_timestamp'].apply(lambda x: (present_day - x).days)\n",
        "  recency_df = recency_df.sort_values(by='Recency', ascending=False)\n",
        "\n",
        "  # todo: frequência\n",
        "  frequency_df = pd.DataFrame(df.groupby([\"customer_unique_id\"]).agg({\"order_id\":\"nunique\"}).reset_index())\n",
        "  frequency_df.rename(columns={\"order_id\":\"Frequency\"}, inplace=True)\n",
        "  frequency_df = frequency_df.sort_values(by='Frequency', ascending=False)\n",
        "\n",
        "  # todo: valor monetário\n",
        "  monetary_df = df.groupby('customer_unique_id', as_index=False)['payment_value'].sum()\n",
        "  monetary_df.columns = ['customer_unique_id', 'Monetary']\n",
        "  monetary_df = monetary_df.sort_values(by='Monetary', ascending=False)\n",
        "\n",
        "  #todo: Resumo de recência, frequência e valor monetário\n",
        "  RF_df = recency_df.merge(frequency_df, on='customer_unique_id')\n",
        "  RFM_df = RF_df.merge(monetary_df, on='customer_unique_id').drop(columns='order_purchase_timestamp')\n",
        "  RFM_estatistica = RFM_df.describe().T\n",
        "\n",
        "\n",
        "  #todo: Remoção de outliers\n",
        "  def num_outlier(df_in, col_name):\n",
        "      q1 = df_in[col_name].quantile(0.05)\n",
        "      q3 = df_in[col_name].quantile(0.95)\n",
        "      iqr = q3-q1 #Interquartile range\n",
        "      fence_low  = q1-1.5*iqr\n",
        "      fence_high = q3+1.5*iqr\n",
        "      outliers_df= df_in.loc[(df_in[col_name] < fence_low) | (df_in[col_name] > fence_high)]\n",
        "      # return print(\"Number of outliers in {} column: \".format(col_name), len(outliers_df)), print(\"Indexes: \", outliers_df.index)\n",
        "\n",
        "  for i in [\"Recency\", \"Frequency\", \"Monetary\"]:\n",
        "      num_outlier(RFM_df, i)\n",
        "\n",
        "  #todo: Remoção de outliers nos 5%\n",
        "  def num_outlier(df_in, col_name):\n",
        "      q1 = df_in[col_name].quantile(0.05)\n",
        "      q3 = df_in[col_name].quantile(0.95)\n",
        "      iqr = q3-q1 #Interquartile range\n",
        "      fence_low  = q1-1.5*iqr\n",
        "      fence_high = q3+1.5*iqr\n",
        "      outliers_df= df_in.loc[(df_in[col_name] < fence_low) | (df_in[col_name] > fence_high)]\n",
        "      # return print(\"Number of outliers in {} column: \".format(col_name), len(outliers_df)), print(\"Indexes: \", outliers_df.index)\n",
        "\n",
        "  for i in [\"Recency\", \"Frequency\", \"Monetary\"]:\n",
        "      num_outlier(RFM_df, i)\n",
        "\n",
        "  RFM_df2= RFM_df.copy()\n",
        "  RFM_df2 = RFM_df2.set_index('customer_unique_id')\n",
        "\n",
        "  RFM_df2[\"recency_score\"]  = pd.qcut(RFM_df2['Recency'], 5, labels=[5, 4, 3, 2, 1])\n",
        "  RFM_df2[\"frequency_score\"]= pd.qcut(RFM_df2['Frequency'].rank(method=\"first\"), 5, labels=[1, 2, 3, 4, 5])\n",
        "  RFM_df2[\"monetary_score\"] = pd.qcut(RFM_df2['Monetary'], 5, labels=[1, 2, 3, 4, 5])\n",
        "  RFM_df2['RFM_SCORE'] = RFM_df2.recency_score.astype(str)+ RFM_df2.frequency_score.astype(str) + RFM_df2.monetary_score.astype(str)\n",
        "\n",
        "  seg_map= {\n",
        "      r'111|112|121|131|141|151': 'Lost customers',\n",
        "      r'332|322|233|232|223|222|132|123|122|212|211': 'Hibernating customers',\n",
        "      r'155|154|144|214|215|115|114|113': 'Cannot Lose Them',\n",
        "      r'255|254|245|244|253|252|243|242|235|234|225|224|153|152|145|143|142|135|134|133|125|124': 'At Risk',\n",
        "      r'331|321|312|221|213|231|241|251': 'About To Sleep',\n",
        "      r'535|534|443|434|343|334|325|324': 'Need Attention',\n",
        "      r'525|524|523|522|521|515|514|513|425|424|413|414|415|315|314|313': 'Promising',\n",
        "      r'512|511|422|421|412|411|311': 'New Customers',\n",
        "      r'553|551|552|541|542|533|532|531|452|451|442|441|431|453|433|432|423|353|352|351|342|341|333|323': 'Potential Loyalist',\n",
        "      r'543|444|435|355|354|345|344|335': 'Loyal',\n",
        "      r'555|554|544|545|454|455|445': 'Champions'\n",
        "  }\n",
        "\n",
        "\n",
        "  RFM_df2['Segment'] = RFM_df2['recency_score'].astype(str) + RFM_df2['frequency_score'].astype(str) + RFM_df2['monetary_score'].astype(str)\n",
        "  RFM_df2['Segment'] = RFM_df2['Segment'].replace(seg_map, regex=True)\n",
        "\n",
        "  RFMStats = RFM_df2[[\"Segment\", \"Recency\", \"Frequency\", \"Monetary\"]].groupby(\"Segment\").agg(['mean','median', 'min', 'max', 'count'])\n",
        "  RFMStats['Ratio']= (100*RFMStats['Monetary'][\"count\"]/RFMStats['Monetary'][\"count\"].sum()).round(2)\n",
        "\n",
        "  RFM_df_raw = RFM_df\n",
        "  RFM_df = RFM_df2\n",
        "  RFM_estatistica = RFMStats\n",
        "\n",
        "\n",
        "  return RFM_df_raw, RFM_df, RFM_estatistica\n",
        "\n",
        "\n",
        "Olist_Cluster_RMF = RMF_Analysis(Olist_dataset['Consolidado'][['order_purchase_timestamp', 'customer_unique_id', \"order_id\", 'payment_value']])\n",
        "\n",
        "widget({'RFM_df_raw': Olist_Cluster_RMF[0],\n",
        "        'RFM_df': Olist_Cluster_RMF[1],\n",
        "        'RFM_estatistica': Olist_Cluster_RMF[2]})\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4PxTzkxrO3Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title 3.1 - RMF Analysis (histograma) - Criar widget\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.subplot(3, 1, 1); sns.distplot(RFM_df['Recency'])\n",
        "plt.subplot(3, 1, 2); sns.distplot(RFM_df['Frequency'])\n",
        "plt.subplot(3, 1, 3); sns.distplot(RFM_df['Monetary'])\n",
        "plt.show()"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-07-23T05:15:48.885563Z",
          "end_time": "2023-07-23T05:15:51.772500Z"
        },
        "id": "bCPG02BP5duj",
        "cellView": "form"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title 3.1 - RMF Analysis - Distribuição de segmentos (ordenar)\n",
        "plt.figure(figsize=(20,8))\n",
        "#plt.rc('font', size=20)\n",
        "per= sns.barplot(x=Olist_Cluster_RMF[2]['Ratio'], y=Olist_Cluster_RMF[2].index, data=Olist_Cluster_RMF[2], palette=\"viridis\")\n",
        "sns.despine(bottom = True, left = True)\n",
        "for i, v in enumerate(Olist_Cluster_RMF[2]['Ratio']):\n",
        "    per.text(v, i+.20,\"  {:.2f}\".format(v)+\"%\", color='black', ha=\"left\")\n",
        "per.set_ylabel('Segments', fontsize=25)\n",
        "per.set(xticks=[])\n",
        "plt.title('Distribution of Segments', fontsize=35)\n",
        "plt.show()"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-07-23T05:16:59.619263Z",
          "end_time": "2023-07-23T05:16:59.792269Z"
        },
        "id": "8kVc_eGK5dui",
        "cellView": "form"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.2 - Customer Satisfaction Prediction\n",
        "# todo: https://towardsdatascience.com/case-study-1-customer-satisfaction-prediction-on-olist-brazillian-dataset-4289bdd20076\n",
        "\n",
        "\n",
        "#The ‘purchase delivery difference’ column gives us the number of days between the time of purchase and delivery.\n",
        "#The ‘estimated actual delivery difference’ column gives us the delay or the cut-down in the number of days required for the delivery.\n",
        "del df\n",
        "df = Olist_dataset['Consolidado']\n",
        "\n",
        "\n",
        "# Make sure the columns are in the correct datetime format\n",
        "df['order_delivered_customer_date'] = pd.to_datetime(Olist_dataset['Consolidado']['order_delivered_customer_date'])\n",
        "df['order_purchase_timestamp'] = pd.to_datetime(Olist_dataset['Consolidado']['order_purchase_timestamp'])\n",
        "df['order_estimated_delivery_date'] = pd.to_datetime(Olist_dataset['Consolidado']['order_estimated_delivery_date'])\n",
        "\n",
        "# Calculate the 'purchase-delivery difference'\n",
        "intermediate_time = df['order_delivered_customer_date'].dt.date - df['order_purchase_timestamp'].dt.date\n",
        "df['purchase-delivery difference'] = intermediate_time.dt.days\n",
        "\n",
        "# Calculate the 'estimated-actual delivery difference'\n",
        "intermediate_time = df['order_estimated_delivery_date'].dt.date - df['order_delivered_customer_date'].dt.date\n",
        "df['estimated-actual delivery difference'] = intermediate_time.dt.days\n",
        "\n",
        "\n",
        "df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eBifPHycZGS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title 3.3 - RMF con KNN e 6 clusters (talvez vá remover)\n",
        "# todo: ML()::RMF::Analysis Segementation\n",
        "#  todo: In this part we want explain our customers segementation, after we apply best method for segementation with Kmeans with 6 customers segementation based on Sillhoutte Score.\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "Olist_resumo_colunas = Olist.resumo_colunas.query('Tabela == \"mvp\"')\n",
        "recency = Olist_dataset['mvp'][['customer_unique_id', 'order_purchase_date']].copy()\n",
        "recency = recency.groupby('customer_unique_id')['order_purchase_date'].max().reset_index()\n",
        "recency.columns = ['customer_unique_id', 'last_purchase_timestamp']\n",
        "# Recency, Frequency, Monetary\n",
        "recency['purchase_int'] = (recency['last_purchase_timestamp'].max() - recency['last_purchase_timestamp']).dt.days\n",
        "recency.drop(columns='last_purchase_timestamp', inplace=True)\n",
        "frequency =pd.DataFrame(Olist_dataset['mvp'].groupby('customer_unique_id')['order_id'].count().reset_index())\n",
        "monetary =pd.DataFrame(Olist_dataset['mvp'].groupby('customer_unique_id')['payment_value'].sum().reset_index())\n",
        "# Merge\n",
        "overall = recency.merge(frequency, on='customer_unique_id')\n",
        "overall = overall.merge(monetary, on='customer_unique_id')\n",
        "\n",
        "# Rename columns for better intepretation\n",
        "overall.rename(columns={ 'purchase_int':'recency', 'num_transaction':'frequency','payment_value':'monetary','order_id':'frequency'}, inplace=True)\n",
        "\n",
        "scaled_features = overall[['customer_unique_id','recency','frequency','monetary']].copy()\n",
        "\n",
        "col_names = ['monetary', 'recency','frequency']\n",
        "features = scaled_features[col_names]\n",
        "scaler = StandardScaler().fit(features.values)\n",
        "features = scaler.transform(features.values)\n",
        "\n",
        "scaled_features[col_names] = features\n",
        "\n",
        "# Segmentation Based on RFM with KMeans\n",
        "kmeans = KMeans(n_clusters=6,random_state= 0)\n",
        "kmeans.fit(scaled_features[['monetary', 'recency','frequency']])\n",
        "scaled_features['k_means_clust'] = kmeans.labels_\n",
        "scaled_features['k_means_clust'].value_counts()\n",
        "\n",
        "#assign customer segmentation based on overall score\n",
        "ksegment_lst = scaled_features['k_means_clust']\n",
        "ksegment_cluster = []\n",
        "\n",
        "for i in ksegment_lst:\n",
        "    if i == 1:\n",
        "        cluster = 'passive'\n",
        "        ksegment_cluster.append(cluster)\n",
        "    elif i == 0:\n",
        "        cluster = 'regular'\n",
        "        ksegment_cluster.append(cluster)\n",
        "    elif i == 3:\n",
        "        cluster = 'occation'\n",
        "        ksegment_cluster.append(cluster)\n",
        "    elif i == 4:\n",
        "        cluster = 'valuable'\n",
        "        ksegment_cluster.append(cluster)\n",
        "    elif i == 2:\n",
        "        cluster = 'loyal'\n",
        "        ksegment_cluster.append(cluster)\n",
        "    else:\n",
        "        cluster = 'best'\n",
        "        ksegment_cluster.append(cluster)\n",
        "\n",
        "scaled_features['k_means_segment'] = ksegment_cluster\n",
        "\n",
        "RMF_1_raw, RMF_1_cluster = overall, scaled_features"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-07-23T05:13:51.974270Z",
          "end_time": "2023-07-23T05:13:52.962821Z"
        },
        "id": "KLUrh4OJ5duh",
        "cellView": "form"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "________________________________________________________________________________\n",
        "## Gráficos"
      ],
      "metadata": {
        "id": "efrzCeAJeL7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Compras por mês\n",
        "\n",
        "def view_compras_por_mes(df, filtro_datas):\n",
        "    if len(filtro_datas) != 2 or filtro_datas[0] is None or filtro_datas[1] is None or filtro_datas[0] > filtro_datas[1]:\n",
        "      print('Selecione um período válido.')\n",
        "      return\n",
        "\n",
        "    hv.extension('bokeh')\n",
        "    filtro = [pd.to_datetime(filtro_datas[0]), pd.to_datetime(filtro_datas[1])]\n",
        "\n",
        "    df = pd.merge(df['orders'], df['order_items'], on=\"order_id\")\n",
        "    df = df[(df['order_purchase_timestamp'] >= filtro[0]) & (df['order_purchase_timestamp'] <= filtro[1])]\n",
        "    vendas = df.groupby(pd.Grouper(key='order_purchase_timestamp', freq='M'))['price'].sum().reset_index()\n",
        "    vendas['price_milhoes'] = vendas['price'] / 1000000\n",
        "\n",
        "    calendario = pd.DataFrame({'order_purchase_timestamp': pd.date_range(start=filtro[0], end=filtro[1], freq='M')})\n",
        "    vendas = calendario.merge(vendas, on='order_purchase_timestamp', how='left')\n",
        "    vendas['price_milhoes'] = vendas['price_milhoes'].fillna(0)\n",
        "\n",
        "    area = hv.Area(vendas, 'order_purchase_timestamp', 'price_milhoes').options(fill_alpha=0.3, color='blue')\n",
        "    curve = hv.Curve(vendas, 'order_purchase_timestamp', 'price_milhoes').options(color='blue')\n",
        "    vlines = hv.Overlay([hv.VLine(mes).options(line_color='lightgray', line_width=1, line_dash='dashed') for mes in vendas['order_purchase_timestamp']])\n",
        "\n",
        "    area.opts(xlabel='Data', ylabel='Vendas (em milhões de reais)', title='Compras mensais da Olist')\n",
        "    curve.opts(xlabel='Data', ylabel='Vendas (em milhões de reais)', title='Compras mensais da Olist')\n",
        "    return (area * curve * vlines).options(height=400, width=800, ylim=(0, vendas['price_milhoes'].max()))\n",
        "\n",
        "\n",
        "view_compras_por_mes(dataset, filtro_datas=['1-2016', '12-2018'])"
      ],
      "metadata": {
        "id": "iqTxroE1eNFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Avaliações e Compras por mês\n",
        "\n",
        "def view_compras_avaliacoes(dataset, filtro_datas):\n",
        "    hv.extension('bokeh')\n",
        "\n",
        "    df = dataset['order_reviews'].merge(dataset['order_items'], on=\"order_id\").merge(dataset['order_payments'], on=\"order_id\").merge(dataset['orders'], on=\"order_id\")\n",
        "    df = df[(df['order_purchase_timestamp'] >= filtro_datas[0]) & (df['order_purchase_timestamp'] <= filtro_datas[1])]\n",
        "\n",
        "    # Agrupar dados por mês e avaliação\n",
        "    vendas = df.groupby([pd.Grouper(key='order_purchase_timestamp', freq='M'), 'review_score'])['payment_value'].sum().reset_index()\n",
        "\n",
        "    # Converter valores para milhões de reais\n",
        "    vendas['price_milhoes'] = vendas['payment_value'] / 1000000\n",
        "\n",
        "    # Formatando a coluna de datas para exibir apenas mês e ano\n",
        "    vendas['order_purchase_timestamp'] = vendas['order_purchase_timestamp'].dt.strftime(\"%m/%y\")\n",
        "\n",
        "    # Criar gráfico de barras empilhadas\n",
        "    bars = hv.Bars(vendas, ['order_purchase_timestamp', 'review_score'], 'price_milhoes').options(\n",
        "        width=800, height=400, stacked=True, cmap='Spectral', legend_position='right'\n",
        "    )\n",
        "    bars.opts(xlabel='Data', ylabel='Vendas (em milhões de reais)', title='Vendas por avaliação da Olist')\n",
        "\n",
        "    return bars\n",
        "\n",
        "\n",
        "view_compras_avaliacoes(dataset, filtro_datas=['1-2016', '12-2018'])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Gof1j11mee4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2chpiDRvkRql"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "v1d86uFJyU61",
        "2QZTf87vylEd",
        "t7fwDG21jslK",
        "mumlyy-2j9bH",
        "L7DBAoykkI7Z",
        "efrzCeAJeL7Z"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}